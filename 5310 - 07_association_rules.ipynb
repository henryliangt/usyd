{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henryliangt/usyd/blob/main/5310%20-%2007_association_rules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBazWnfkXffK"
      },
      "source": [
        "# Data mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvU5dR_HXffQ"
      },
      "source": [
        "## EXERCISE: Association analysis from scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN_755L7XffR"
      },
      "source": [
        "### Generate frequent itemsets\n",
        "\n",
        "Let's find all sets of items with a support greater than some threshold.\n",
        "\n",
        "We define 4 functions for generating frequent itemsets:\n",
        "* createC1 - Create first candidate itemsets for k=1\n",
        "* scanD - Identify itemsets that meet the support threshold\n",
        "* aprioriGen - Generate the next list of candidates\n",
        "* apriori - Generate all frequent itemsets\n",
        "\n",
        "See slides for explanation of functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDqDTVqhXffT"
      },
      "outputs": [],
      "source": [
        "def createC1(dataset):\n",
        "    \"Create a list of candidate item sets of size one.\"\n",
        "    c1 = []\n",
        "    for transaction in dataset:\n",
        "        for item in transaction:\n",
        "            if not [item] in c1:\n",
        "                c1.append([item])\n",
        "    c1.sort()\n",
        "    #frozenset because it will be a ket of a dictionary.                         \n",
        "    return list(map(frozenset, c1))\n",
        "\n",
        "\n",
        "\n",
        "def scanD(dataset, candidates, min_support):\n",
        "    \"Returns all candidates that meets a minimum support level\"\n",
        "    sscnt = {}\n",
        "    for tid in dataset:\n",
        "        for can in candidates:\n",
        "            if can.issubset(tid):\n",
        "                sscnt.setdefault(can, 0)\n",
        "                sscnt[can] += 1\n",
        "\n",
        "    num_items = float(len(dataset))\n",
        "    retlist = []\n",
        "    support_data = {}\n",
        "    for key in sscnt:\n",
        "        support = sscnt[key] / num_items\n",
        "        if support >= min_support:\n",
        "            retlist.insert(0, key)\n",
        "            support_data[key] = support\n",
        "    return retlist, support_data\n",
        "\n",
        "\n",
        "def aprioriGen(freq_sets, k):\n",
        "    \"Generate the joint transactions from candidate sets\"\n",
        "    retList = []\n",
        "    lenLk = len(freq_sets)\n",
        "    for i in range(lenLk):\n",
        "        for j in range(i + 1, lenLk):\n",
        "            L1 = list(freq_sets[i])[:k - 2]\n",
        "            L2 = list(freq_sets[j])[:k - 2]\n",
        "            L1.sort()\n",
        "            L2.sort()\n",
        "            if L1 == L2:\n",
        "                retList.append(freq_sets[i] | freq_sets[j]) # | is set union\n",
        "    return retList\n",
        "\n",
        "\n",
        "def apriori(dataset, min_support=0.5):\n",
        "    \"Generate a list of candidate item sets\"\n",
        "    C1 = createC1(dataset)\n",
        "    D = list(map(set, dataset))\n",
        "    L1, support_data = scanD(D, C1, min_support)\n",
        "    L = [L1]\n",
        "    k = 2\n",
        "    while (len(L[k - 2]) > 0):\n",
        "        Ck = aprioriGen(L[k - 2], k)\n",
        "        Lk, supK = scanD(D, Ck, min_support)\n",
        "        support_data.update(supK)\n",
        "        L.append(Lk)\n",
        "        k += 1\n",
        "\n",
        "    return L, support_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_ad = 'https://github.com/henryliangt/usyd/blob/main/Groceries.csv'\n",
        "\n",
        "!pip install pyfpgrowth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yguYkHj6Xwew",
        "outputId": "74d18b55-c7dc-4a0b-9a0c-58b991d0e3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyfpgrowth\n",
            "  Downloading pyfpgrowth-1.0.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 7.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyfpgrowth\n",
            "  Building wheel for pyfpgrowth (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfpgrowth: filename=pyfpgrowth-1.0-py2.py3-none-any.whl size=5504 sha256=e40ad80000c2e88f7c3c1d032428b417b8e1076cfe3570b41b0bb5eaeb3b7ca5\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/97/4b/f12ac994f6bbb99597396255435824c73ad3916be1e678be55\n",
            "Successfully built pyfpgrowth\n",
            "Installing collected packages: pyfpgrowth\n",
            "Successfully installed pyfpgrowth-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpgrowth_py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN6fBT8QYIP5",
        "outputId": "cbfc0a7c-6e66-47f2-f68c-698101fae131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fpgrowth_py\n",
            "  Downloading fpgrowth_py-1.0.0-py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: fpgrowth-py\n",
            "Successfully installed fpgrowth-py-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlxtend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9cKP38sYNib",
        "outputId": "83ba7149-f73e-463e-fb89-f4c6211edf02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.0.2)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.3.5)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.7.3)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->mlxtend) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.1->mlxtend) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->mlxtend) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apriori_python"
      ],
      "metadata": {
        "id": "rWR07gAPYQAe",
        "outputId": "83688a45-c46f-4f47-ee43-233e63b462d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting apriori_python\n",
            "  Downloading apriori_python-1.0.4-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: apriori-python\n",
            "Successfully installed apriori-python-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV3BTfvKXffW"
      },
      "source": [
        "### Itemset generation on sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tQ5R9IkXffW"
      },
      "outputs": [],
      "source": [
        "MIN_SUPPORT= 0.5\n",
        "\n",
        "# Sample data\n",
        "DATASET = [['Mango', 'Onion', 'Apple'], ['Corn', 'Onion', 'Eggs'], ['Mango', 'Corn', 'Onion', 'Eggs'], ['Mango', 'Eggs']]\n",
        "DATASET = [['A', 'C', 'D'], ['B', 'C', 'E'], ['A', 'B', 'C','E'],['B', 'E']]\n",
        "print('Dataset in list-of-lists format:\\n', DATASET, '\\n')\n",
        "\n",
        "# Generate a first candidate itemsets for k=1\n",
        "C1 = createC1(DATASET)\n",
        "print('Initial 1-itemset candidates:\\n', C1, '\\n')\n",
        "\n",
        "# Convert data to a list of sets\n",
        "D = list(map(set, DATASET))\n",
        "print('Dataset in list-of-sets format:\\n', D, '\\n')\n",
        "\n",
        "# Identify items that meet support threshold (0.5)\n",
        "# Note that {4} isn't here as it only occurs in one transaction.\n",
        "# Remove it so we don't generate any further candidate itemsets containing {4}.\n",
        "L1, support_data = scanD(D, C1, MIN_SUPPORT)\n",
        "print('1-itemsets that appear in at least 50% of transactions:\\n', L1, '\\n')\n",
        "\n",
        "# Generate the next list of candidates\n",
        "print('Next set of candidates:\\n', aprioriGen(L1,2), '\\n')\n",
        "\n",
        "# Generate all candidate itemsets\n",
        "L, support_data = apriori(DATASET, min_support=MIN_SUPPORT)\n",
        "print('Full list of candidate itemsets:\\n', L, '\\n')\n",
        "print('Support values for candidate itemsets:\\n', support_data, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8GbGW0hXffX"
      },
      "source": [
        "### TODO Exploring support thresholds\n",
        "\n",
        "* Generate frequent itemsets with a support threshold of 0.7\n",
        "* How many frequent itemsets do we get at 0.7?\n",
        "* How many do we get at 0.3?\n",
        "* Do you have datasets that resemble transactions?\n",
        "* What about the apps/websites you use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnhZ_tcsXffY"
      },
      "outputs": [],
      "source": [
        "# TODO: replace the content of this cell with your Python solution\n",
        "L7, support_data07 = apriori(DATASET, min_support=0.7)\n",
        "print('frequ items: ' , L7)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIvtufQ3XffZ"
      },
      "source": [
        "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LQjtOHnXffc"
      },
      "source": [
        "## Mine association rules\n",
        "\n",
        "Given frequent itemsets, we can create association rules.\n",
        "\n",
        "We add three more functions:\n",
        "* calc_confidence - Identify rules that meet the confidence threshold\n",
        "* rules_from_conseq - Recursively generate and evaluate candidate rules\n",
        "* generateRules - Mine all confident association rules\n",
        "\n",
        "See slides for explanation of functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJZU6R6GXffd"
      },
      "outputs": [],
      "source": [
        "def calc_confidence(freqSet, H, support_data, rules, min_confidence=0.7):\n",
        "    \"Evaluate the rule generated\"\n",
        "    pruned_H = []\n",
        "    for conseq in H:\n",
        "        conf = support_data[freqSet] / support_data[freqSet - conseq]\n",
        "        if conf >= min_confidence:\n",
        "            #print(freqSet - conseq, '--->', conseq, 'conf:', conf)\n",
        "            rules.append((freqSet - conseq, conseq, conf))\n",
        "            pruned_H.append(conseq)\n",
        "    return pruned_H\n",
        "\n",
        "\n",
        "def rules_from_conseq(freqSet, H, support_data, rules, min_confidence=0.7):\n",
        "    \"Generate a set of candidate rules\"\n",
        "    m = len(H[0])\n",
        "    Hmp1 = createC1(H)\n",
        "    Hmp1 = calc_confidence(freqSet, Hmp1,  support_data, rules, min_confidence)\n",
        "    if len(Hmp1) <= len(freqSet):\n",
        "        if (len(freqSet) > (m + 1)):\n",
        "            Hmp1 = aprioriGen(H, m + 1)\n",
        "            Hmp1 = calc_confidence(freqSet, Hmp1,  support_data, rules, min_confidence)\n",
        "            if len(Hmp1) > 1:\n",
        "                rules_from_conseq(freqSet, Hmp1, support_data, rules, min_confidence)\n",
        "\n",
        "def generateRules(L, support_data, min_confidence=0.7):\n",
        "    \"\"\"Create the association rules\n",
        "    L: list of frequent item sets\n",
        "    support_data: support data for those itemsets\n",
        "    min_confidence: minimum confidence threshold\n",
        "    \"\"\"\n",
        "    rules = []\n",
        "    for i in range(1, len(L)):\n",
        "        for freqSet in L[i]:\n",
        "            H1 = [frozenset([item]) for item in freqSet]           \n",
        "            if (i > 1):\n",
        "                rules_from_conseq(freqSet, H1, support_data, rules, min_confidence)\n",
        "            else:\n",
        "                calc_confidence(freqSet, H1, support_data, rules, min_confidence)\n",
        "    return rules\n",
        "\n",
        "def print_rules(rules):\n",
        "    for r in rules:\n",
        "        print('{} ==> {} (c={})'.format(*r))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB84dAwKXfff"
      },
      "source": [
        "### Rule mining on sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzhLEDEcXfff"
      },
      "outputs": [],
      "source": [
        "\n",
        "MIN_CONFIDENCE = 0.7\n",
        "# Mine association rules\n",
        "association_rules = generateRules(L, support_data, min_confidence=MIN_CONFIDENCE)\n",
        "print_rules(association_rules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeWV3LizXffg"
      },
      "source": [
        "### TODO Exploring confidence thresholds\n",
        "\n",
        "* Mine rules with a confidence threshold of 0.7\n",
        "* How many rules do we get at 0.7?\n",
        "* How many do we get at 0.3?\n",
        "* Can we use this for recommendation (e.g., Amazon, Netflix)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoSyJAclXffg"
      },
      "outputs": [],
      "source": [
        "# TODO: replace the content of this cell with your Python solution\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLpI-Ui7Xffh"
      },
      "source": [
        "## EXERCISE: mlxtend library and apriori_python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaZZCzo-Xffh"
      },
      "source": [
        "## Association analysis using mlxtend library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4YJu3J5Xffi"
      },
      "outputs": [],
      "source": [
        "#Install the library if it is not availab\n",
        "#!pip install mlxtend\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "import pandas as pd\n",
        "dataset =   [['A', 'C', 'D'], ['B', 'C', 'E'], ['A', 'B', 'C','E'],['B', 'E']]\n",
        "            \n",
        "             \n",
        "oht = TransactionEncoder()\n",
        "oht_ary = oht.fit(dataset).transform(dataset)\n",
        "df = pd.DataFrame(oht_ary, columns=oht.columns_)\n",
        "print (df)           \n",
        " \n",
        "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
        "print('Support values for candidate itemsets:\\n', frequent_itemsets, '\\n')\n",
        "\n",
        " \n",
        "rules= association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "#rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
        "#print (rules.as_matrix(columns=['antecedents','consequents','confidence']))\n",
        "print(rules[{'antecedents','consequents','confidence'}])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyOWXtuVXffj"
      },
      "source": [
        "## Association analysis using apriori_python library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHhM7tWxXffj"
      },
      "outputs": [],
      "source": [
        "#!pip install apriori_python\n",
        "from apriori_python import apriori\n",
        "dataset =   [['A', 'C', 'D'], ['B', 'C', 'E'], ['A', 'B', 'C','E'],['B', 'E']]\n",
        "freqItemSet, rules = apriori(dataset, minSup=0.7, minConf=0.0)\n",
        "\n",
        "for r in rules:\n",
        "    print('{} ==> {} (c={})'.format(*r)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSy4HmHwXffk"
      },
      "source": [
        "# Load the  supermarket transaction datasets\n",
        "### Now Lets work on a real Grocery dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgTEJTmWXffk"
      },
      "outputs": [],
      "source": [
        "import csv \n",
        "import pprint\n",
        "file_name = 'Groceries.csv'\n",
        "data_list = []\n",
        "with open(file_name, 'r') as f:  #opens PW file\n",
        "    reader = csv.reader(f)\n",
        "    # Print every value of every row. \n",
        "    for row in reader:\n",
        "        row_list = []\n",
        "        for value in row: \n",
        "            if len(value.strip()) > 0 and value.strip() != '':\n",
        "                row_list.append(value.strip())\n",
        "        data_list.append(row_list)\n",
        "pprint.pprint(data_list)        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfpKzRhRXffk"
      },
      "source": [
        "## TODO Mining association rules on Groceries datasets\n",
        "* Apply apriori and association_rules functions from mlxtend library\n",
        "* Apply apriori and association_rules functions from apriori_python library\n",
        "* What would be a reasonable value of min-support for these supermarket transaction data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_DVsoLUXffl"
      },
      "outputs": [],
      "source": [
        "# TODO: replace the content of this cell with your Python solution\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQLAY5RcXffl"
      },
      "source": [
        "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD0j1cdkXffm"
      },
      "source": [
        "# EXERCISE: FP-Growth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpTysvnKXffm"
      },
      "source": [
        "## Rules  generation using pyfpgrowth library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENvjKG3vXffm"
      },
      "outputs": [],
      "source": [
        "#Install the library if it is not available\n",
        "#!pip install pyfpgrowth\n",
        "import pyfpgrowth\n",
        "MIN_SUPPORT = 2 \n",
        "MIN_CONFIDENCE = 0.7\n",
        "DATASET = [['A', 'C', 'D'], ['B', 'C', 'E'], ['A', 'B', 'C','E'],['B', 'E']]\n",
        "\n",
        "frequent_itemsets = pyfpgrowth.find_frequent_patterns(DATASET, MIN_SUPPORT)\n",
        "print('Support values for candidate itemsets:\\n', frequent_itemsets, '\\n')\n",
        "rules = pyfpgrowth.generate_association_rules(frequent_itemsets, MIN_CONFIDENCE)\n",
        "print('Resultant assoication rules:\\n')\n",
        "pprint.pprint(rules) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDIPBa5AXffn"
      },
      "source": [
        "## Rules  generation using fpgrowth_py library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCmrVOGwXffn"
      },
      "outputs": [],
      "source": [
        "#!pip install fpgrowth_py\n",
        "from fpgrowth_py import fpgrowth\n",
        "freqItemSet, rules = fpgrowth(DATASET, minSupRatio=0.5, minConf=0.7)\n",
        "for r in rules:\n",
        "    print('{} ==> {} (c={})'.format(*r)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ehnu3ZXffn"
      },
      "source": [
        "### TODO Mining association rules using FP-growth and fpgrowth_py on Groceries datasets\n",
        "* Try different confidence thresholds\n",
        "* What’s a reasonable value for real data?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FPDVcjxXffo"
      },
      "outputs": [],
      "source": [
        "# TODO: replace the content of this cell with your Python solution\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMunRzWRXffo"
      },
      "source": [
        "# End of Tutorial. Many Thanks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvI5XX0lXffo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}